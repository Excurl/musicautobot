{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp music_transformer.dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Fastai Language Model Databunch modified to work with music'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#export\n",
    "\"Fastai Language Model Databunch modified to work with music\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from musicautobot.imports import *\n",
    "from musicautobot.music_transformer.transform import *\n",
    "from musicautobot.vocab import MusicVocab\n",
    "\n",
    "import random\n",
    "from fastai.basics import *\n",
    "from fastai.text.all import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = untar_data(URLs.WIKITEXT_TINY)\n",
    "# df_train = pd.read_csv(path/'train.csv', header=None)\n",
    "# df_valid = pd.read_csv(path/'test.csv', header=None)\n",
    "# all_texts = np.concatenate([df_train[0].values, df_valid[0].values])\n",
    "\n",
    "# from fastai.text.all import *\n",
    "\n",
    "# class TransformersTokenizer(Transform):\n",
    "#     def __init__(self, tokenizer): self.tokenizer = tokenizer\n",
    "#     def encodes(self, x): \n",
    "#         toks = self.tokenizer.tokenize(x)\n",
    "#         return tensor(self.tokenizer.convert_tokens_to_ids(toks))\n",
    "#     def decodes(self, x): return TitledStr(self.tokenizer.decode(x.cpu().numpy()))\n",
    "\n",
    "# from transformers import GPT2LMHeadModel, GPT2TokenizerFast\n",
    "\n",
    "# pretrained_weights = 'gpt2'\n",
    "# tokenizer = GPT2TokenizerFast.from_pretrained(pretrained_weights)\n",
    "\n",
    "# splits = [range_of(df_train), list(range(len(df_train), len(all_texts)))]\n",
    "# tls = TfmdLists(all_texts, TransformersTokenizer(tokenizer), splits=splits, dl_type=LMDataLoader)\n",
    "\n",
    "# bs,sl = 4,256\n",
    "# dls = tls.dataloaders(bs=bs, seq_len=sl)\n",
    "\n",
    "# dls.train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Music Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = Path('../../data')\n",
    "midi_path = base_path/'midi/examples'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "midi_files = get_files(midi_path, '.mid', recurse=True); len(midi_files)\n",
    "vocab = MusicVocab.create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Midi2ItemTfm(Transform):\n",
    "    \"Skips midi preprocessing step. And encodes midi files to MusicItems\"\n",
    "    def __init__(self,vocab):\n",
    "        self.vocab = vocab\n",
    "    \n",
    "    def encodes(self, o):\n",
    "        return MusicItem.from_file(o, vocab=self.vocab)\n",
    "    \n",
    "class MusicItemTfm(Transform):\n",
    "    \"`PreProcessor` that transforms numpy files to indexes for training\"\n",
    "    def __init__(self,vocab):\n",
    "        self.vocab = vocab\n",
    "        \n",
    "    def encodes(self, f):\n",
    "        npitem = np.load(f, allow_pickle=True) if isinstance(f, Path) else f\n",
    "        miitem = MusicItem.from_npenc(npitem, vocab=self.vocab)\n",
    "        return miitem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def mi2tensor(mi): return np.stack([mi.data, mi.position]).T\n",
    "\n",
    "def rand_transpose(mi, steps=6, p=0.5):\n",
    "    if random.random() >= p: return mi\n",
    "    val = random.randint(-steps, steps)\n",
    "    return mi.transpose(val)\n",
    "\n",
    "def batch_position_tfm(b):\n",
    "    \"Batch transform for training with positional encoding\"\n",
    "    x,y = b\n",
    "    x = {\n",
    "        'x': x[...,0],\n",
    "        'pos': x[...,1]\n",
    "    }\n",
    "    return x, y[...,0]\n",
    "\n",
    "@delegates()\n",
    "class MusicItemDataLoader(LMDataLoader):\n",
    "    def __init__(self, dataset, trange=6, tp=0.5, encode_position=False, **kwargs):\n",
    "        store_attr('trange,tp,encode_position')\n",
    "        super().__init__(dataset, **kwargs)\n",
    "        \n",
    "    def make_chunks(self): \n",
    "        transpose_tfm = partial(rand_transpose, steps=self.trange, p=self.tp)\n",
    "        tensor_tfm = mi2tensor if self.encode_position else lambda x: x.data\n",
    "        pipeline = Pipeline([transpose_tfm, tensor_tfm])\n",
    "        \n",
    "        self.chunks = Chunks(list(map(pipeline, self.items)), self.lens)\n",
    "        \n",
    "    def create_item(self, seq):\n",
    "        item = super().create_item(seq)\n",
    "        return batch_position_tfm(item) if self.encode_position else item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: Make mi2tensor and rand_transpose into a pipeline transform.\n",
    "## It no longer needs to be inside dataloader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms = [Midi2ItemTfm(vocab)]\n",
    "splits = RandomSplitter(seed=42)(range(len(midi_files)))\n",
    "dsets = Datasets(midi_files, [tfms], splits=splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LMTensorText([[  0,   1,  71, 139,  66, 145,  62, 145,  59, 145]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dls = DataLoaders.from_dsets(dsets, dl_type=MusicItemDataLoader, bs=1, seq_len=13)\n",
    "xb,yb = dls.one_batch()\n",
    "xb['x'][:,:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Sanity Check\n",
    "# music_items = [MusicItem.from_file(f, vocab=vocab) for f in midi_files]\n",
    "# tensor_items = [mi2tensor(mi) for mi in music_items]\n",
    "# lens = [len(mi) for mi in music_items]\n",
    "# cs = Chunks(tensor_items, lens)\n",
    "# cs[1:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted config.ipynb.\n",
      "Converted dataloader.ipynb.\n",
      "Converted learner.ipynb.\n",
      "Converted model.ipynb.\n",
      "Converted transform.ipynb.\n",
      "Converted Train.ipynb.\n",
      "Converted dataloader-reference.ipynb.\n",
      "Converted dataloader-v1.ipynb.\n",
      "Converted dataloader.ipynb.\n",
      "Converted learner.ipynb.\n",
      "Converted model.ipynb.\n",
      "Converted transform.ipynb.\n",
      "Converted numpy_encode.ipynb.\n",
      "Converted attention_mask.ipynb.\n",
      "Converted env_setup.ipynb.\n",
      "Converted file_processing.ipynb.\n",
      "Converted lamb.ipynb.\n",
      "Converted midifile.ipynb.\n",
      "Converted stacked_dataloader.ipynb.\n",
      "Converted top_k_top_p.ipynb.\n",
      "Converted vocab.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script(recursive=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
