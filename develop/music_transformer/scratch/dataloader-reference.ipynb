{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Fastai Language Model Databunch modified to work with music'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Fastai Language Model Databunch modified to work with music\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from fastai.basics import *\n",
    "# from fastai.basic_data import DataBunch\n",
    "# from fastai.text.data import LMLabelList\n",
    "\n",
    "from musicautobot.music_transformer.transform import *\n",
    "from musicautobot.vocab import MusicVocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.text.data import *\n",
    "from fastai.basics import *\n",
    "from fastai.text.all import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = untar_data(URLs.WIKITEXT_TINY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(path/'train.csv', header=None)\n",
    "df_valid = pd.read_csv(path/'test.csv', header=None)\n",
    "df_all = pd.concat([df_train, df_valid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ashaw/anaconda3/envs/musicautobot/lib/python3.8/site-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    }
   ],
   "source": [
    "splits = [list(range_of(df_train)), list(range(len(df_train), len(df_all)))]\n",
    "tfms = [attrgetter(\"text\"), Tokenizer.from_df(0), Numericalize()]\n",
    "dsets = Datasets(df_all, [tfms], splits=splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = LMDataLoader(dsets, bs=12, seq_len=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorText([  2,  17, 444,  ...,  52,   8, 872])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dls.items[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs = Chunks(dls.items, dls.lens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(\"Hello world!\", return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 7592, 2088,  999,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bs,sl = 104,512\n",
    "# dls = dsets.dataloaders(bs=bs, seq_len=sl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Music Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = Path('../../data')\n",
    "\n",
    "# Location of your midi files\n",
    "midi_path = base_path/'midi/examples'\n",
    "midi_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Location to save dataset\n",
    "data_path = base_path/'numpy'\n",
    "data_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "data_save_name = 'musicitem_data_save.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "midi_files = get_files(midi_path, '.mid', recurse=True); len(midi_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = MusicVocab.create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = RandomSplitter(seed=42)(range(len(midi_files)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Midi2ItemProcessor(Transform):\n",
    "    \"Skips midi preprocessing step. And encodes midi files to MusicItems\"\n",
    "    def __init__(self,vocab):\n",
    "        self.vocab = vocab\n",
    "    \n",
    "    def encodes(self, o):\n",
    "        return MusicItem.from_file(o, vocab=self.vocab)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms = [Midi2ItemProcessor(vocab)]\n",
    "dsets = Datasets(midi_files, [tfms], splits=splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "music_items = [MusicItem.from_file(f, vocab=vocab) for f in midi_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MusicTensor():\n",
    "#     def __init__(self, data, vocab, transpose=0):\n",
    "#         self.data = data\n",
    "#         self.vocab = vocab\n",
    "#     @classmethod\n",
    "#     def from_item(cls, mi):\n",
    "#         return cls(np.stack([mi.data, mi.position]).T, mi.vocab)\n",
    "    \n",
    "#     def __getitem__(self,i):\n",
    "#         return self.data[i]\n",
    "#         if isinstance(i,slice): return retain_type(self.getslice(i), old=self.chunks[0])\n",
    "#         di,idx = self.doc_idx(i)\n",
    "#         return retain_type(self.chunks[di][idx], old=self.chunks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-4"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.randint(-6, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7875462847226555"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.random()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mi2tensor(mi): return np.stack([mi.data, mi.position]).T\n",
    "\n",
    "def rand_transpose(mi, steps=6, p=0.5):\n",
    "    if random.random() >= p: return mi\n",
    "    val = random.randint(-steps, steps)\n",
    "    return mi.transpose(val)\n",
    "\n",
    "class MusicItemDataLoader(LMDataLoader):\n",
    "    def make_chunks(self): \n",
    "        items = [mi2tensor(rand_transpose(mi)) for mi in self.items]\n",
    "        self.chunks = Chunks(items, self.lens)\n",
    "    def shuffle_fn(self,idxs):\n",
    "        self.items.shuffle()\n",
    "        self.make_chunks()\n",
    "        return idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Chunks:\n",
    "    \"Slice and int indexing into a list of lists\"\n",
    "    def __init__(self, chunks, lens=None):\n",
    "        self.chunks = chunks\n",
    "        self.lens = L(map(len,self.chunks) if lens is None else lens)\n",
    "        self.cumlens = np.cumsum(0+self.lens)\n",
    "        self.totlen = self.cumlens[-1]\n",
    "\n",
    "    def __getitem__(self,i):\n",
    "        if isinstance(i,slice): return retain_type(self.getslice(i), old=self.chunks[0])\n",
    "        di,idx = self.doc_idx(i)\n",
    "        return retain_type(self.chunks[di][idx], old=self.chunks[0])\n",
    "\n",
    "    def getslice(self, i):\n",
    "        st_d,st_i = self.doc_idx(ifnone(i.start,0))\n",
    "        en_d,en_i = self.doc_idx(ifnone(i.stop,self.totlen+1))\n",
    "        res = [self.chunks[st_d][st_i:(en_i if st_d==en_d else sys.maxsize)]]\n",
    "        for b in range(st_d+1,en_d): res.append(self.chunks[b])\n",
    "        if st_d!=en_d and en_d<len(self.chunks): res.append(self.chunks[en_d][:en_i])\n",
    "        return concat(*res)\n",
    "\n",
    "    def doc_idx(self, i):\n",
    "        if i<0: i=self.totlen+i # count from end\n",
    "        docidx = np.searchsorted(self.cumlens, i+1)-1\n",
    "        cl = self.cumlens[docidx]\n",
    "        return docidx,i-cl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReindexCollection(GetAttr, IterLen):\n",
    "    \"Reindexes collection `coll` with indices `idxs` and optional LRU cache of size `cache`\"\n",
    "    _default='coll'\n",
    "    def __init__(self, coll, idxs=None, cache=None, tfm=noop):\n",
    "        if idxs is None: idxs = L.range(coll)\n",
    "        store_attr()\n",
    "        if cache is not None: self._get = functools.lru_cache(maxsize=cache)(self._get)\n",
    "\n",
    "    def _get(self, i): return self.tfm(self.coll[i])\n",
    "    def __getitem__(self, i): return self._get(self.idxs[i])\n",
    "    def __len__(self): return len(self.coll)\n",
    "    def reindex(self, idxs): self.idxs = idxs\n",
    "    def shuffle(self): random.shuffle(self.idxs)\n",
    "    def cache_clear(self): self._get.cache_clear()\n",
    "    def __getstate__(self): return {'coll': self.coll, 'idxs': self.idxs, 'cache': self.cache, 'tfm': self.tfm}\n",
    "    def __setstate__(self, s): self.coll,self.idxs,self.cache,self.tfm = s['coll'],s['idxs'],s['cache'],s['tfm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@delegates()\n",
    "class LMDataLoader(TfmdDL):\n",
    "    \"A `DataLoader` suitable for language modeling\"\n",
    "    def __init__(self, dataset, lens=None, cache=2, bs=64, seq_len=72, num_workers=0, **kwargs):\n",
    "        self.items = ReindexCollection(dataset, cache=cache, tfm=_maybe_first)\n",
    "        self.seq_len = seq_len\n",
    "        if lens is None: lens = _get_lengths(dataset)\n",
    "        if lens is None: lens = [len(o) for o in self.items]\n",
    "        self.lens = ReindexCollection(lens, idxs=self.items.idxs)\n",
    "        # The \"-1\" is to allow for final label, we throw away the end that's less than bs\n",
    "        corpus = round_multiple(sum(lens)-1, bs, round_down=True)\n",
    "        self.bl = corpus//bs #bl stands for batch length\n",
    "        self.n_batches = self.bl//(seq_len) + int(self.bl%seq_len!=0)\n",
    "        self.last_len = self.bl - (self.n_batches-1)*seq_len\n",
    "        self.make_chunks()\n",
    "        super().__init__(dataset=dataset, bs=bs, num_workers=num_workers, **kwargs)\n",
    "        self.n = self.n_batches*bs\n",
    "\n",
    "    def make_chunks(self): self.chunks = Chunks(self.items, self.lens)\n",
    "    def shuffle_fn(self,idxs):\n",
    "        self.items.shuffle()\n",
    "        self.make_chunks()\n",
    "        return idxs\n",
    "\n",
    "    def create_item(self, seq):\n",
    "        if seq>=self.n: raise IndexError\n",
    "        sl = self.last_len if seq//self.bs==self.n_batches-1 else self.seq_len\n",
    "        st = (seq%self.bs)*self.bl + (seq//self.bs)*self.seq_len\n",
    "        txt = self.chunks[st : st+sl+1]\n",
    "        return LMTensorText(txt[:-1]),txt[1:]\n",
    "\n",
    "    @delegates(TfmdDL.new)\n",
    "    def new(self, dataset=None, seq_len=None, **kwargs):\n",
    "        lens = self.lens.coll if dataset is None else None\n",
    "        seq_len = self.seq_len if seq_len is None else seq_len\n",
    "        return super().new(dataset=dataset, lens=lens, seq_len=seq_len, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   1,  69, 141,  66, 141,  62, 141,   8, 141,  69, 141,   8,\n",
       "       141,  61, 138,   8, 138,  64, 139,   8, 139,  69, 139,   8, 139,\n",
       "        64, 140,   8, 138,  64, 139,  61, 139,  57, 139,   8, 141,  59,\n",
       "       139,  56, 139,  52, 139,   8, 139,  59, 141,  56, 141,  52, 141,\n",
       "         8, 139,  61, 139,   8, 139,  61, 138,   8, 138,  64, 139,   8,\n",
       "       139,  73, 140,   8, 140,  69, 141,  61, 139,  57, 139,  54, 139,\n",
       "         8, 141,  69, 139,  66, 139,  62, 139,   8, 139,  69, 141,  66,\n",
       "       141,  62, 141,   8, 139,  69, 139,   8, 139,  73, 139,   8, 139,\n",
       "        74, 139,   8, 139,  73, 139,   8, 139,  69, 139,  64, 139,  61,\n",
       "       139,  57, 139,   8, 141,  59, 139,  56, 139,  52, 139,   8, 139,\n",
       "        59, 141,  56, 141,  52, 141,   8, 139,  69, 139,   8, 139,  73,\n",
       "       139,   8, 139,  71, 139,   8, 139,  71, 139,   8, 139,  69, 139])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "music_items[0].data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "mi = music_items[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(156, 2)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.stack([mi.data, mi.position]).T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_items = [np.stack([mi.data, mi.position]).T for mi in music_items]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "lens = [len(mi) for mi in music_items]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs = Chunks(tensor_items, lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  4,  4,  4,  4,  8,  8,\n",
       "         8,  8,  9,  9,  9,  9, 11, 11, 11, 11, 13, 13, 13, 13, 14, 14,\n",
       "        14, 14, 14, 14, 14, 14, 18, 18, 18, 18, 18, 18, 18, 18, 20, 20,\n",
       "        20, 20, 20, 20, 20, 20, 22, 22, 22, 22, 24, 24, 24, 24, 25, 25,\n",
       "        25, 25, 27, 27, 27, 27, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30,\n",
       "        34, 34, 34, 34, 34, 34, 34, 34, 36, 36, 36, 36, 36, 36, 36, 36,\n",
       "        38, 38, 38, 38, 40, 40, 40, 40, 42, 42, 42, 42, 44, 44, 44, 44,\n",
       "        46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 50, 50, 50, 50, 50, 50,\n",
       "        50, 50, 52, 52, 52, 52, 52, 52, 52, 52, 54, 54, 54, 54, 56, 56,\n",
       "        56, 56, 58, 58, 58, 58, 60, 60, 60, 60, 62, 62]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_items[0][]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1,   0],\n",
       "       [ 69,   0],\n",
       "       [141,   0],\n",
       "       [ 66,   0],\n",
       "       [141,   0],\n",
       "       [ 62,   0],\n",
       "       [141,   0],\n",
       "       [  8,   0],\n",
       "       [141,   0],\n",
       "       [ 69,   4],\n",
       "       [141,   4],\n",
       "       [  8,   4],\n",
       "       [141,   4],\n",
       "       [ 61,   8],\n",
       "       [138,   8],\n",
       "       [  8,   8],\n",
       "       [138,   8],\n",
       "       [ 64,   9],\n",
       "       [139,   9],\n",
       "       [  8,   9],\n",
       "       [139,   9],\n",
       "       [ 69,  11],\n",
       "       [139,  11],\n",
       "       [  8,  11],\n",
       "       [139,  11],\n",
       "       [ 64,  13],\n",
       "       [140,  13],\n",
       "       [  8,  13],\n",
       "       [138,  13],\n",
       "       [ 64,  14],\n",
       "       [139,  14],\n",
       "       [ 61,  14],\n",
       "       [139,  14],\n",
       "       [ 57,  14],\n",
       "       [139,  14],\n",
       "       [  8,  14],\n",
       "       [141,  14],\n",
       "       [ 59,  18],\n",
       "       [139,  18],\n",
       "       [ 56,  18],\n",
       "       [139,  18],\n",
       "       [ 52,  18],\n",
       "       [139,  18],\n",
       "       [  8,  18],\n",
       "       [139,  18],\n",
       "       [ 59,  20],\n",
       "       [141,  20],\n",
       "       [ 56,  20],\n",
       "       [141,  20],\n",
       "       [ 52,  20],\n",
       "       [141,  20],\n",
       "       [  8,  20],\n",
       "       [139,  20],\n",
       "       [ 61,  22],\n",
       "       [139,  22],\n",
       "       [  8,  22],\n",
       "       [139,  22],\n",
       "       [ 61,  24],\n",
       "       [138,  24],\n",
       "       [  8,  24],\n",
       "       [138,  24],\n",
       "       [ 64,  25],\n",
       "       [139,  25],\n",
       "       [  8,  25],\n",
       "       [139,  25],\n",
       "       [ 73,  27],\n",
       "       [140,  27],\n",
       "       [  8,  27],\n",
       "       [140,  27],\n",
       "       [ 69,  30],\n",
       "       [141,  30],\n",
       "       [ 61,  30],\n",
       "       [139,  30],\n",
       "       [ 57,  30],\n",
       "       [139,  30],\n",
       "       [ 54,  30],\n",
       "       [139,  30],\n",
       "       [  8,  30],\n",
       "       [141,  30],\n",
       "       [ 69,  34],\n",
       "       [139,  34],\n",
       "       [ 66,  34],\n",
       "       [139,  34],\n",
       "       [ 62,  34],\n",
       "       [139,  34],\n",
       "       [  8,  34],\n",
       "       [139,  34],\n",
       "       [ 69,  36],\n",
       "       [141,  36],\n",
       "       [ 66,  36],\n",
       "       [141,  36],\n",
       "       [ 62,  36],\n",
       "       [141,  36],\n",
       "       [  8,  36],\n",
       "       [139,  36],\n",
       "       [ 69,  38],\n",
       "       [139,  38],\n",
       "       [  8,  38],\n",
       "       [139,  38]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cs[1:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MusicItemProcessor(PreProcessor):\n",
    "    \"`PreProcessor` that transforms numpy files to indexes for training\"\n",
    "    def process_one(self,item):\n",
    "        item = MusicItem.from_npenc(item, vocab=self.vocab)\n",
    "        return item.to_idx()\n",
    "    \n",
    "    def process(self, ds):\n",
    "        self.vocab = ds.vocab\n",
    "        super().process(ds)\n",
    "        \n",
    "class OpenNPFileProcessor(PreProcessor):\n",
    "    \"`PreProcessor` that opens the filenames and read the texts.\"\n",
    "    def process_one(self,item):\n",
    "        return np.load(item, allow_pickle=True) if isinstance(item, Path) else item\n",
    "\n",
    "class Midi2ItemProcessor(PreProcessor):\n",
    "    \"Skips midi preprocessing step. And encodes midi files to MusicItems\"\n",
    "    def process_one(self,item):\n",
    "        item = MusicItem.from_file(item, vocab=self.vocab)\n",
    "        return item.to_idx()\n",
    "    \n",
    "    def process(self, ds):\n",
    "        self.vocab = ds.vocab\n",
    "        super().process(ds)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object `Midi2ItemProcessor` not found.\n"
     ]
    }
   ],
   "source": [
    "??Midi2ItemProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processors = [Midi2ItemProcessor()]\n",
    "# data = MusicDataBunch.from_files(midi_files, data_path, processors=processors, bs=2, bptt=12)\n",
    "# data.save(data_save_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Chunks:\n",
    "    \"Slice and int indexing into a list of lists\"\n",
    "    def __init__(self, chunks, lens=None):\n",
    "        self.chunks = chunks\n",
    "        self.lens = L(map(len,self.chunks) if lens is None else lens)\n",
    "        self.cumlens = np.cumsum(0+self.lens)\n",
    "        self.totlen = self.cumlens[-1]\n",
    "\n",
    "    def __getitem__(self,i):\n",
    "        if isinstance(i,slice): return retain_type(self.getslice(i), old=self.chunks[0])\n",
    "        di,idx = self.doc_idx(i)\n",
    "        return retain_type(self.chunks[di][idx], old=self.chunks[0])\n",
    "\n",
    "    def getslice(self, i):\n",
    "        st_d,st_i = self.doc_idx(ifnone(i.start,0))\n",
    "        en_d,en_i = self.doc_idx(ifnone(i.stop,self.totlen+1))\n",
    "        res = [self.chunks[st_d][st_i:(en_i if st_d==en_d else sys.maxsize)]]\n",
    "        for b in range(st_d+1,en_d): res.append(self.chunks[b])\n",
    "        if st_d!=en_d and en_d<len(self.chunks): res.append(self.chunks[en_d][:en_i])\n",
    "        return concat(*res)\n",
    "\n",
    "    def doc_idx(self, i):\n",
    "        if i<0: i=self.totlen+i # count from end\n",
    "        docidx = np.searchsorted(self.cumlens, i+1)-1\n",
    "        cl = self.cumlens[docidx]\n",
    "        return docidx,i-cl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "??Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<fastai.torch_core.Chunks at 0x7f94d2519eb0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " 'chunks',\n",
       " 'cumlens',\n",
       " 'doc_idx',\n",
       " 'getslice',\n",
       " 'lens',\n",
       " 'totlen']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(cs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "??Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>xxbos = 2013 – 14 xxmaj york xxmaj city xxup f.c . season = \\n▁\\n▁ xxmaj the 2013 – 14 season was the xxunk season of competitive association football and 77th season in the xxmaj football xxmaj league played by xxmaj york xxmaj city xxmaj football xxmaj club , a professional football club based in xxmaj york , xxmaj north xxmaj yorkshire , xxmaj england . xxmaj their 17th - place finish in 2012 – 13 meant it was their second consecutive season in xxmaj league xxmaj two . xxmaj the season ran from 1 xxmaj july 2013 to 30 xxmaj june 2014 . \\n▁ xxmaj nigel xxmaj worthington , starting his first full season as xxmaj york manager , made eight permanent summer signings . xxmaj by the turn of the year xxmaj york were only above the relegation zone on goal difference , before a xxunk unbeaten run</td>\n",
       "      <td>= 2013 – 14 xxmaj york xxmaj city xxup f.c . season = \\n▁\\n▁ xxmaj the 2013 – 14 season was the xxunk season of competitive association football and 77th season in the xxmaj football xxmaj league played by xxmaj york xxmaj city xxmaj football xxmaj club , a professional football club based in xxmaj york , xxmaj north xxmaj yorkshire , xxmaj england . xxmaj their 17th - place finish in 2012 – 13 meant it was their second consecutive season in xxmaj league xxmaj two . xxmaj the season ran from 1 xxmaj july 2013 to 30 xxmaj june 2014 . \\n▁ xxmaj nigel xxmaj worthington , starting his first full season as xxmaj york manager , made eight permanent summer signings . xxmaj by the turn of the year xxmaj york were only above the relegation zone on goal difference , before a xxunk unbeaten run saw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>. ) \\n▁ xxmaj pulaski 's apprehension at using the transporter was evident in \" xxmaj the xxunk xxmaj man \" , where xxmaj dr . xxunk ( xxmaj suzie xxmaj plakson ) went with the away team instead of xxmaj pulaski , as it required her to beam over to a transport vessel . xxmaj however , the transporter would later save xxmaj pulaski 's life in \" xxmaj unnatural xxmaj selection \" , after she was infected with a disease from the planet xxunk xxup iv that accelerated her aging process . xxmaj she uses the xxunk to remove the infection and is returned to health . \\n▁ xxmaj she demonstrates her medical expertise on several occasions . xxmaj in \" xxmaj time xxunk \" , xxmaj pulaski discovers that the duplicate xxmaj captain xxmaj picard is out of sync in time and will slowly improve until he</td>\n",
       "      <td>) \\n▁ xxmaj pulaski 's apprehension at using the transporter was evident in \" xxmaj the xxunk xxmaj man \" , where xxmaj dr . xxunk ( xxmaj suzie xxmaj plakson ) went with the away team instead of xxmaj pulaski , as it required her to beam over to a transport vessel . xxmaj however , the transporter would later save xxmaj pulaski 's life in \" xxmaj unnatural xxmaj selection \" , after she was infected with a disease from the planet xxunk xxup iv that accelerated her aging process . xxmaj she uses the xxunk to remove the infection and is returned to health . \\n▁ xxmaj she demonstrates her medical expertise on several occasions . xxmaj in \" xxmaj time xxunk \" , xxmaj pulaski discovers that the duplicate xxmaj captain xxmaj picard is out of sync in time and will slowly improve until he returns</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>xxmaj his xxmaj prime xxunk lasted barely a year , marred by problems and difficulties . xxmaj for the remainder of his life and without his wife , as xxmaj queen xxmaj victoria xxunk it , \" to hold him back \" , he became more and more eccentric and controversial in his decisions . xxmaj his final years were xxunk by ill health and a self - enforced seclusion in xxmaj scotland . xxmaj he died in 1929 . \\n▁ xxmaj before their marriage and his full - time entry into politics , xxmaj rosebery 's future wife had written with extraordinary xxunk and ambition to him : \" i work only to help you , if you are xxmaj prime xxmaj minister , let me imitate xxmaj montagu xxunk . \" xxunk had been xxmaj disraeli 's influential private secretary on whom he had relied . xxmaj rosebery</td>\n",
       "      <td>his xxmaj prime xxunk lasted barely a year , marred by problems and difficulties . xxmaj for the remainder of his life and without his wife , as xxmaj queen xxmaj victoria xxunk it , \" to hold him back \" , he became more and more eccentric and controversial in his decisions . xxmaj his final years were xxunk by ill health and a self - enforced seclusion in xxmaj scotland . xxmaj he died in 1929 . \\n▁ xxmaj before their marriage and his full - time entry into politics , xxmaj rosebery 's future wife had written with extraordinary xxunk and ambition to him : \" i work only to help you , if you are xxmaj prime xxmaj minister , let me imitate xxmaj montagu xxunk . \" xxunk had been xxmaj disraeli 's influential private secretary on whom he had relied . xxmaj rosebery only</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls.show_batch(max_n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "xb,yb = dls.one_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "??ReindexCollection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DataBunch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-f0945e075373>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mMusicDataBunch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDataBunch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;34m\"Create a `TextDataBunch` suitable for training a language model.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     def create(cls, train_ds, valid_ds, test_ds=None, path:PathOrStr='.', no_check:bool=False, bs=64, val_bs:int=None, \n\u001b[1;32m      5\u001b[0m                \u001b[0mnum_workers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mCallable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_collate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DataBunch' is not defined"
     ]
    }
   ],
   "source": [
    "class MusicDataBunch(DataBunch):\n",
    "    \"Create a `TextDataBunch` suitable for training a language model.\"\n",
    "    @classmethod\n",
    "    def create(cls, train_ds, valid_ds, test_ds=None, path:PathOrStr='.', no_check:bool=False, bs=64, val_bs:int=None, \n",
    "               num_workers:int=0, device:torch.device=None, collate_fn:Callable=data_collate, \n",
    "               dl_tfms:Optional[Collection[Callable]]=None, bptt:int=70,\n",
    "               preloader_cls=None, shuffle_dl=False, transpose_range=(0,12), **kwargs) -> DataBunch:\n",
    "        \"Create a `TextDataBunch` in `path` from the `datasets` for language modelling.\"\n",
    "        datasets = cls._init_ds(train_ds, valid_ds, test_ds)\n",
    "        preloader_cls = MusicPreloader if preloader_cls is None else preloader_cls\n",
    "        val_bs = ifnone(val_bs, bs)\n",
    "        datasets = [preloader_cls(ds, shuffle=(i==0), bs=(bs if i==0 else val_bs), bptt=bptt, transpose_range=transpose_range, **kwargs) \n",
    "                    for i,ds in enumerate(datasets)]\n",
    "        val_bs = bs\n",
    "        dl_tfms = [partially_apply_vocab(tfm, train_ds.vocab) for tfm in listify(dl_tfms)]\n",
    "        dls = [DataLoader(d, b, shuffle=shuffle_dl) for d,b in zip(datasets, (bs,val_bs,val_bs,val_bs)) if d is not None]\n",
    "        return cls(*dls, path=path, device=device, dl_tfms=dl_tfms, collate_fn=collate_fn, no_check=no_check)\n",
    "    \n",
    "    @classmethod    \n",
    "    def from_folder(cls, path:PathOrStr, extensions='.npy', **kwargs):\n",
    "        files = get_files(path, extensions=extensions, recurse=True);\n",
    "        return cls.from_files(files, path, **kwargs)\n",
    "    \n",
    "    @classmethod\n",
    "    def from_files(cls, files, path, processors=None, split_pct=0.1, \n",
    "                   vocab=None, list_cls=None, **kwargs):\n",
    "        if vocab is None: vocab = MusicVocab.create()\n",
    "        if list_cls is None: list_cls = MusicItemList\n",
    "        src = (list_cls(items=files, path=path, processor=processors, vocab=vocab)\n",
    "                .split_by_rand_pct(split_pct, seed=6)\n",
    "                .label_const(label_cls=LMLabelList))\n",
    "        return src.databunch(**kwargs)\n",
    "\n",
    "    @classmethod\n",
    "    def empty(cls, path, **kwargs):\n",
    "        vocab = MusicVocab.create()\n",
    "        src = MusicItemList([], path=path, vocab=vocab, ignore_empty=True).split_none()\n",
    "        return src.label_const(label_cls=LMLabelList).databunch()\n",
    "        \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ItemList' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-6d074bfaf7fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtfm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mMusicItemList\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mItemList\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0m_bunch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMusicDataBunch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ItemList' is not defined"
     ]
    }
   ],
   "source": [
    "   \n",
    "def partially_apply_vocab(tfm, vocab):\n",
    "    if 'vocab' in inspect.getfullargspec(tfm).args:\n",
    "        return partial(tfm, vocab=vocab)\n",
    "    return tfm\n",
    "    \n",
    "class MusicItemList(ItemList):\n",
    "    _bunch = MusicDataBunch\n",
    "    \n",
    "    def __init__(self, items:Iterator, vocab:MusicVocab=None, **kwargs):\n",
    "        super().__init__(items, **kwargs)\n",
    "        self.vocab = vocab\n",
    "        self.copy_new += ['vocab']\n",
    "    \n",
    "    def get(self, i):\n",
    "        o = super().get(i)\n",
    "        if is_pos_enc(o): \n",
    "            return MusicItem.from_idx(o, self.vocab)\n",
    "        return MusicItem(o, self.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_pos_enc(idxenc):\n",
    "    if len(idxenc.shape) == 2 and idxenc.shape[0] == 2: return True\n",
    "    return idxenc.dtype == np.object and idxenc.shape == (2,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MusicItemProcessor(PreProcessor):\n",
    "    \"`PreProcessor` that transforms numpy files to indexes for training\"\n",
    "    def process_one(self,item):\n",
    "        item = MusicItem.from_npenc(item, vocab=self.vocab)\n",
    "        return item.to_idx()\n",
    "    \n",
    "    def process(self, ds):\n",
    "        self.vocab = ds.vocab\n",
    "        super().process(ds)\n",
    "        \n",
    "class OpenNPFileProcessor(PreProcessor):\n",
    "    \"`PreProcessor` that opens the filenames and read the texts.\"\n",
    "    def process_one(self,item):\n",
    "        return np.load(item, allow_pickle=True) if isinstance(item, Path) else item\n",
    "\n",
    "class Midi2ItemProcessor(PreProcessor):\n",
    "    \"Skips midi preprocessing step. And encodes midi files to MusicItems\"\n",
    "    def process_one(self,item):\n",
    "        item = MusicItem.from_file(item, vocab=self.vocab)\n",
    "        return item.to_idx()\n",
    "    \n",
    "    def process(self, ds):\n",
    "        self.vocab = ds.vocab\n",
    "        super().process(ds)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For npenc dataset\n",
    "class MusicPreloader(Callback):\n",
    "    \"Transforms the tokens in `dataset` to a stream of contiguous batches for language modelling.\"\n",
    "    \n",
    "    class CircularIndex():\n",
    "        \"Handles shuffle, direction of indexing, wraps around to head tail in the ragged array as needed\"\n",
    "        def __init__(self, length:int, forward:bool): self.idx, self.forward = np.arange(length), forward\n",
    "        def __getitem__(self, i): \n",
    "            return self.idx[ i%len(self.idx) if self.forward else len(self.idx)-1-i%len(self.idx)]\n",
    "        def __len__(self) -> int: return len(self.idx)\n",
    "        def shuffle(self): np.random.shuffle(self.idx)\n",
    "\n",
    "    def __init__(self, dataset:LabelList, lengths:Collection[int]=None, bs:int=32, bptt:int=70, backwards:bool=False, \n",
    "                 shuffle:bool=False, y_offset:int=1, \n",
    "                 transpose_range=None, transpose_p=0.5,\n",
    "                 encode_position=True,\n",
    "                 **kwargs):\n",
    "        self.dataset,self.bs,self.bptt,self.shuffle,self.backwards,self.lengths = dataset,bs,bptt,shuffle,backwards,lengths\n",
    "        self.vocab = self.dataset.vocab\n",
    "        self.bs *= num_distrib() or 1\n",
    "        self.totalToks,self.ite_len,self.idx = int(0),None,None\n",
    "        self.y_offset = y_offset\n",
    "        \n",
    "        self.transpose_range,self.transpose_p = transpose_range,transpose_p\n",
    "        self.encode_position = encode_position\n",
    "        self.bptt_len = self.bptt\n",
    "        \n",
    "        self.allocate_buffers() # needed for valid_dl on distributed training - otherwise doesn't get initialized on first epoch\n",
    "\n",
    "    def __len__(self): \n",
    "        if self.ite_len is None:\n",
    "            if self.lengths is None: self.lengths = np.array([len(item) for item in self.dataset.x])\n",
    "            self.totalToks = self.lengths.sum()\n",
    "            self.ite_len   = self.bs*int( math.ceil( self.totalToks/(self.bptt*self.bs) )) if self.item is None else 1\n",
    "        return self.ite_len\n",
    "\n",
    "    def __getattr__(self,k:str)->Any: return getattr(self.dataset, k)\n",
    "   \n",
    "    def allocate_buffers(self):\n",
    "        \"Create the ragged array that will be filled when we ask for items.\"\n",
    "        if self.ite_len is None: len(self)\n",
    "        self.idx   = MusicPreloader.CircularIndex(len(self.dataset.x), not self.backwards)\n",
    "        \n",
    "        # batch shape = (bs, bptt, 2 - [index, pos]) if encode_position. Else - (bs, bptt)\n",
    "        buffer_len = (2,) if self.encode_position else ()\n",
    "        self.batch = np.zeros((self.bs, self.bptt+self.y_offset) + buffer_len, dtype=np.int64)\n",
    "        self.batch_x, self.batch_y = self.batch[:,0:self.bptt], self.batch[:,self.y_offset:self.bptt+self.y_offset] \n",
    "        #ro: index of the text we're at inside our datasets for the various batches\n",
    "        self.ro    = np.zeros(self.bs, dtype=np.int64)\n",
    "        #ri: index of the token we're at inside our current text for the various batches\n",
    "        self.ri    = np.zeros(self.bs, dtype=np.int)\n",
    "        \n",
    "        # allocate random transpose values. Need to allocate this before hand.\n",
    "        self.transpose_values = self.get_random_transpose_values()\n",
    "        \n",
    "    def get_random_transpose_values(self):\n",
    "        if self.transpose_range is None: return None\n",
    "        n = len(self.dataset)\n",
    "        rt_arr = torch.randint(*self.transpose_range, (n,))-self.transpose_range[1]//2\n",
    "        mask = torch.rand(rt_arr.shape) > self.transpose_p\n",
    "        rt_arr[mask] = 0\n",
    "        return rt_arr\n",
    "\n",
    "    def on_epoch_begin(self, **kwargs):\n",
    "        if self.idx is None: self.allocate_buffers()\n",
    "        elif self.shuffle:   \n",
    "            self.ite_len = None\n",
    "            self.idx.shuffle()\n",
    "            self.transpose_values = self.get_random_transpose_values()\n",
    "            self.bptt_len = self.bptt\n",
    "        self.idx.forward = not self.backwards \n",
    "\n",
    "        step = self.totalToks / self.bs\n",
    "        ln_rag, countTokens, i_rag = 0, 0, -1\n",
    "        for i in range(0,self.bs):\n",
    "            #Compute the initial values for ro and ri \n",
    "            while ln_rag + countTokens <= int(step * i):\n",
    "                countTokens += ln_rag\n",
    "                i_rag       += 1\n",
    "                ln_rag       = self.lengths[self.idx[i_rag]]\n",
    "            self.ro[i] = i_rag\n",
    "            self.ri[i] = ( ln_rag - int(step * i - countTokens) ) if self.backwards else int(step * i - countTokens)\n",
    "        \n",
    "    #Training dl gets on_epoch_begin called, val_dl, on_epoch_end\n",
    "    def on_epoch_end(self, **kwargs): self.on_epoch_begin()\n",
    "\n",
    "    def __getitem__(self, k:int):\n",
    "        j = k % self.bs\n",
    "        if j==0:\n",
    "            if self.item is not None: return self.dataset[0]\n",
    "            if self.idx is None: self.on_epoch_begin()\n",
    "                \n",
    "        self.ro[j],self.ri[j] = self.fill_row(not self.backwards, self.dataset.x, self.idx, self.batch[j][:self.bptt_len+self.y_offset], \n",
    "                                              self.ro[j], self.ri[j], overlap=1, lengths=self.lengths)\n",
    "        return self.batch_x[j][:self.bptt_len], self.batch_y[j][:self.bptt_len]\n",
    "\n",
    "    def fill_row(self, forward, items, idx, row, ro, ri, overlap, lengths):\n",
    "        \"Fill the row with tokens from the ragged array. --OBS-- overlap != 1 has not been implemented\"\n",
    "        ibuf = n = 0 \n",
    "        ro  -= 1\n",
    "        while ibuf < row.shape[0]:  \n",
    "            ro   += 1 \n",
    "            ix    = idx[ro]\n",
    "            \n",
    "            item = items[ix]\n",
    "            if self.transpose_values is not None: \n",
    "                item = item.transpose(self.transpose_values[ix].item())\n",
    "                \n",
    "            if self.encode_position:\n",
    "                # Positions are colomn stacked with indexes. This makes it easier to keep in sync\n",
    "                rag = np.stack([item.data, item.position], axis=1)\n",
    "            else:\n",
    "                rag = item.data\n",
    "                \n",
    "            if forward:\n",
    "                ri = 0 if ibuf else ri\n",
    "                n  = min(lengths[ix] - ri, row.shape[0] - ibuf)\n",
    "                row[ibuf:ibuf+n] = rag[ri:ri+n]\n",
    "            else:    \n",
    "                ri = lengths[ix] if ibuf else ri\n",
    "                n  = min(ri, row.size - ibuf) \n",
    "                row[ibuf:ibuf+n] = rag[ri-n:ri][::-1]\n",
    "            ibuf += n\n",
    "        return ro, ri + ((n-overlap) if forward else -(n-overlap))\n",
    "\n",
    "def batch_position_tfm(b):\n",
    "    \"Batch transform for training with positional encoding\"\n",
    "    x,y = b\n",
    "    x = {\n",
    "        'x': x[...,0],\n",
    "        'pos': x[...,1]\n",
    "    }\n",
    "    return x, y[...,0]\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
