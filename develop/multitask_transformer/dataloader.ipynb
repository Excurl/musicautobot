{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp multitask_transformer.dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from fastai.basics import *\n",
    "\n",
    "from musicautobot.multitask_transformer.transform import *\n",
    "from musicautobot.music_transformer.dataloader import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400, 400)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from musicautobot.vocab import *\n",
    "\n",
    "base_path = Path('../../data/v20')\n",
    "\n",
    "# Location of your midi files\n",
    "midi_path = base_path/'midi_sources/hooktheory'\n",
    "\n",
    "# Location to save dataset\n",
    "s2s_path = base_path/'s2s_encode/hooktheory'\n",
    "lm_path = base_path/'piano_duet/hooktheory'\n",
    "\n",
    "vocab = MusicVocab.create()\n",
    "s2s_files = get_files(s2s_path, '.npy', recurse=True)[:400]\n",
    "lm_files = get_files(lm_path, '.npy', recurse=True)[:400]\n",
    "\n",
    "len(s2s_files), len(lm_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2a. Create NextWord/Mask Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "# DATALOADING AND TRANSFORMATIONS\n",
    "# These transforms happen on batch\n",
    "\n",
    "def mask_tfm(b, mask_range, mask_idx, pad_idx, p=0.3):\n",
    "    # mask range (min, max)\n",
    "    # replacement vals - [x_replace, y_replace]. Usually [mask_idx, pad_idx]\n",
    "    # p = replacement probability\n",
    "    x,y = b\n",
    "    x,y = x.clone(),y.clone()\n",
    "    rand = torch.rand(x.shape, device=x.device)\n",
    "    rand[x < mask_range[0]] = 1.0\n",
    "    rand[x >= mask_range[1]] = 1.0\n",
    "    \n",
    "    # p(15%) of words are replaced. Of those p(15%) - 80% are masked. 10% wrong word. 10% unchanged\n",
    "    y[rand > p] = pad_idx # pad unchanged 80%. Remove these from loss/acc metrics\n",
    "    x[rand <= (p*.8)] = mask_idx # 80% = mask\n",
    "    wrong_word = (rand > (p*.8)) & (rand <= (p*.9)) # 10% = wrong word\n",
    "    x[wrong_word] = torch.randint(*mask_range, [wrong_word.sum().item()], device=x.device)\n",
    "    return x, y\n",
    "\n",
    "def mask_lm_tfm_default(b, vocab, mask_p=0.3):\n",
    "    return mask_lm_tfm(b, mask_range=vocab.npenc_range, mask_idx=vocab.mask_idx, pad_idx=vocab.pad_idx, mask_p=mask_p)\n",
    "\n",
    "def mask_lm_tfm_pitchdur(b, vocab, mask_p=0.9):\n",
    "    mask_range = vocab.dur_range if np.random.rand() < 0.5 else vocab.note_range\n",
    "    return mask_lm_tfm(b, mask_range=mask_range, mask_idx=vocab.mask_idx, pad_idx=vocab.pad_idx, mask_p=mask_p)\n",
    "\n",
    "def mask_lm_tfm(b, mask_range, mask_idx, pad_idx, mask_p):\n",
    "    x,y = b\n",
    "    x_lm,x_pos = x[...,0], x[...,1]\n",
    "    y_lm,y_pos = y[...,0], y[...,1]\n",
    "    \n",
    "    # Note: masking y_lm instead of x_lm. Just in case we ever do sequential s2s training\n",
    "    x_msk, y_msk = mask_tfm((y_lm, y_lm), mask_range=mask_range, mask_idx=mask_idx, pad_idx=pad_idx, p=mask_p)\n",
    "    msk_pos = y_pos\n",
    "    \n",
    "    x_dict = { \n",
    "        'msk': { 'x': x_msk, 'pos': msk_pos },\n",
    "        'lm': { 'x': x_lm, 'pos': msk_pos }\n",
    "    }\n",
    "    y_dict = { 'msk': y_msk, 'lm': y_lm }\n",
    "    return x_dict, y_dict\n",
    "\n",
    "\n",
    "class MaskLMTransform(ItemTransform):\n",
    "    def __init__(self, vocab, mask_p=0.5):\n",
    "        self.vocab = vocab\n",
    "        self.mask_p = mask_p\n",
    "        \n",
    "    def encodes(self, b):\n",
    "        vocab = self.vocab\n",
    "        return mask_lm_tfm(b, mask_range=vocab.npenc_range, mask_idx=vocab.mask_idx, pad_idx=vocab.pad_idx, mask_p=self.mask_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.text.all import LMDataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfms = [MusicItemTfm(vocab), rand_transpose, lambda x: x.data]\n",
    "tfms = [MusicItemTfm(vocab), rand_transpose, mi2tensor]\n",
    "splits = RandomSplitter(seed=42)(range(len(lm_files)))\n",
    "dsets = Datasets(lm_files, [tfms], splits=splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 16\n",
    "seq_len = 512\n",
    "\n",
    "batch_tfms = [MaskLMTransform(vocab)]\n",
    "dls = dsets.dataloaders(dl_type=LMDataLoader, \n",
    "                        bs=bs, seq_len=seq_len, cache=bs*4,\n",
    "                        after_batch=batch_tfms,\n",
    "                       ).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 512])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb,yb = dls.one_batch()\n",
    "xb['msk']['x'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "# Sequence 2 Sequence Translate\n",
    "\n",
    "class S2SFileTfm(Transform):\n",
    "    def __init__(self, vocab):\n",
    "        self.vocab = vocab\n",
    "    \n",
    "    def encodes(self, item):\n",
    "        m,c = np.load(item, allow_pickle=True)\n",
    "        return MultitrackItem.from_npenc_parts(m, c, vocab=self.vocab)\n",
    "\n",
    "class Midi2MultitrackTfm(Transform):\n",
    "    \"Converts midi files to multitrack items\"\n",
    "    def __init__(self, vocab):\n",
    "        self.vocab = vocab\n",
    "        \n",
    "    def encodes(self, midi_file):\n",
    "        try:\n",
    "            item = MultitrackItem.from_file(midi_file, vocab=self.vocab)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            return None\n",
    "        return item\n",
    "\n",
    "def mtt2tensor(mtt, seq_len):\n",
    "    item = mtt.pad_to(seq_len+1)\n",
    "    ((m_x, m_pos), (c_x, c_pos)) = item.to_idx()\n",
    "    return m_x, m_pos, c_x, c_pos\n",
    "\n",
    "def melody_chord_tfm(b):\n",
    "    m,m_pos,c,c_pos = b\n",
    "    \n",
    "    # offset x and y for next word prediction\n",
    "    y_m = m[:,1:]\n",
    "    x_m, m_pos = m[:,:-1], m_pos[:,:-1]\n",
    "    \n",
    "    y_c = c[:,1:]\n",
    "    x_c, c_pos = c[:,:-1], c_pos[:,:-1]\n",
    "    \n",
    "    x_dict = { \n",
    "        'c2m': {\n",
    "            'enc': x_c,\n",
    "            'enc_pos': c_pos,\n",
    "            'dec': x_m,\n",
    "            'dec_pos': m_pos\n",
    "        },\n",
    "        'm2c': {\n",
    "            'enc': x_m,\n",
    "            'enc_pos': m_pos,\n",
    "            'dec': x_c,\n",
    "            'dec_pos': c_pos\n",
    "        }\n",
    "    }\n",
    "    y_dict = {\n",
    "        'c2m': y_m, 'm2c': y_c\n",
    "    }\n",
    "    return x_dict, y_dict\n",
    "\n",
    "class M2CTransform(ItemTransform):\n",
    "    def encodes(self, b):\n",
    "        return melody_chord_tfm(b[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs,seq_len = 2,8\n",
    "tfms = [S2SFileTfm(vocab), partial(mtt2tensor, seq_len=seq_len)]\n",
    "# tfms = [S2SFileTfm(vocab), rand_transpose, mtt2tensor]\n",
    "splits = RandomSplitter(seed=42)(range(len(s2s_files)))\n",
    "dsets = Datasets(s2s_files, [tfms], splits=splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_tfms = [M2CTransform()]\n",
    "dls = dsets.dataloaders(bs=bs, seq_len=seq_len,\n",
    "                        after_batch=batch_tfms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'c2m': {'enc': tensor([[  5,   1,   8, 169,  59, 145,  56, 145],\n",
       "          [  5,   1,  69, 145,  66, 145,  62, 145]], device='cuda:0'),\n",
       "  'enc_pos': tensor([[ 0,  0,  0,  0, 32, 32, 32, 32],\n",
       "          [ 0,  0,  0,  0,  0,  0,  0,  0]], device='cuda:0'),\n",
       "  'dec': tensor([[  6,   1,  76, 139,   8, 139,  76, 139],\n",
       "          [  6,   1,  74, 139,   8, 139,  78, 139]], device='cuda:0'),\n",
       "  'dec_pos': tensor([[0, 0, 0, 0, 0, 0, 2, 2],\n",
       "          [0, 0, 0, 0, 0, 0, 2, 2]], device='cuda:0')},\n",
       " 'm2c': {'enc': tensor([[  6,   1,  76, 139,   8, 139,  76, 139],\n",
       "          [  6,   1,  74, 139,   8, 139,  78, 139]], device='cuda:0'),\n",
       "  'enc_pos': tensor([[0, 0, 0, 0, 0, 0, 2, 2],\n",
       "          [0, 0, 0, 0, 0, 0, 2, 2]], device='cuda:0'),\n",
       "  'dec': tensor([[  5,   1,   8, 169,  59, 145,  56, 145],\n",
       "          [  5,   1,  69, 145,  66, 145,  62, 145]], device='cuda:0'),\n",
       "  'dec_pos': tensor([[ 0,  0,  0,  0, 32, 32, 32, 32],\n",
       "          [ 0,  0,  0,  0,  0,  0,  0,  0]], device='cuda:0')}}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb, yb = dls.one_batch(); xb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted config.ipynb.\n",
      "Converted Train-before_cleanup.ipynb.\n",
      "Converted Train.ipynb.\n",
      "Converted dataloader.ipynb.\n",
      "Converted learner.ipynb.\n",
      "Converted model.ipynb.\n",
      "Converted transform.ipynb.\n",
      "Converted Train.ipynb.\n",
      "Converted dataloader.ipynb.\n",
      "Converted learner.ipynb.\n",
      "Converted model.ipynb.\n",
      "Converted Train-Scratch.ipynb.\n",
      "Converted dataloader-reference.ipynb.\n",
      "Converted dataloader-v1.ipynb.\n",
      "Converted transform.ipynb.\n",
      "Converted numpy_encode.ipynb.\n",
      "Converted attention_mask.ipynb.\n",
      "Converted env_setup.ipynb.\n",
      "Converted fastai_transformer.ipynb.\n",
      "Converted file_processing.ipynb.\n",
      "Converted lamb.ipynb.\n",
      "Converted midifile.ipynb.\n",
      "Converted stacked_dataloader.ipynb.\n",
      "Converted top_k_top_p.ipynb.\n",
      "Converted vocab.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script(recursive=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
