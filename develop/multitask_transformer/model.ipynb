{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp multitask_transformer.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from typing import Collection, Tuple\n",
    "from fastai.basics import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taken from fastai_v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def dropout_mask(x:Tensor, sz:Collection[int], p:float):\n",
    "    \"Return a dropout mask of the same type as `x`, size `sz`, with probability `p` to cancel an element.\"\n",
    "    return x.new(*sz).bernoulli_(1-p).div_(1-p)\n",
    "\n",
    "class RNNDropout(Module):\n",
    "    \"Dropout with probability `p` that is consistent on the seq_len dimension.\"\n",
    "\n",
    "    def __init__(self, p:float=0.5): self.p=p\n",
    "\n",
    "    def forward(self, x:Tensor)->Tensor:\n",
    "        if not self.training or self.p == 0.: return x\n",
    "        m = dropout_mask(x.data, (x.size(0), 1, x.size(2)), self.p)\n",
    "        return x * m\n",
    "\n",
    "\n",
    "Activation = Enum('Activation', 'ReLU Swish GeLU')\n",
    "\n",
    "\n",
    "class PositionalEncoding(Module):\n",
    "    \"Encode the position with a sinusoid.\"\n",
    "    def __init__(self, d:int): self.register_buffer('freq', 1 / (10000 ** (torch.arange(0., d, 2.)/d)))\n",
    "\n",
    "    def forward(self, pos:Tensor):\n",
    "        inp = torch.ger(pos, self.freq)\n",
    "        enc = torch.cat([inp.sin(), inp.cos()], dim=-1)\n",
    "        return enc\n",
    "\n",
    "class GeLU(Module):\n",
    "    def forward(self, x): return 0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n",
    "\n",
    "class Swish(Module):\n",
    "    def forward(self, x): return x * torch.sigmoid(x)\n",
    "\n",
    "_activ_func = {Activation.ReLU:nn.ReLU(inplace=True), Activation.GeLU:GeLU(), Activation.Swish: Swish()}\n",
    "\n",
    "def feed_forward(d_model:int, d_ff:int, ff_p:float=0., act:Activation=Activation.ReLU, double_drop:bool=True):\n",
    "    layers = [nn.Linear(d_model, d_ff), _activ_func[act]]\n",
    "    if double_drop: layers.append(nn.Dropout(ff_p))\n",
    "    return SequentialEx(*layers, nn.Linear(d_ff, d_model), nn.Dropout(ff_p), MergeLayer(), nn.LayerNorm(d_model))\n",
    "\n",
    "\n",
    "def _line_shift(x:Tensor, mask:bool=False):\n",
    "    \"Shift the line i of `x` by p-i elements to the left, is `mask` puts 0s on the diagonal.\"\n",
    "    bs,nh,n,p = x.size()\n",
    "    x_pad = torch.cat([x.new_zeros(bs,nh,n,1), x], dim=3)\n",
    "    x_shift = x_pad.view(bs,nh,p + 1,n)[:,:,1:].view_as(x)\n",
    "    if mask: x_shift.mul_(torch.tril(x.new_ones(n,p), p-n)[None,None,])\n",
    "    return x_shift\n",
    "\n",
    "\n",
    "def init_transformer(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Linear') != -1:\n",
    "        if hasattr(m, 'weight') and m.weight is not None: nn.init.normal_(m.weight, 0., 0.02)\n",
    "        if hasattr(m, 'bias') and m.bias is not None:     nn.init.constant_(m.bias, 0.)\n",
    "    elif classname.find('LayerNorm') != -1:\n",
    "        if hasattr(m, 'weight') and m.weight is not None: nn.init.normal_(m.weight, 1., 0.02)\n",
    "        if hasattr(m, 'bias') and m.bias is not None:     nn.init.constant_(m.bias, 0.)\n",
    "    elif classname.find('TransformerXL') != -1:\n",
    "        if hasattr(m, 'u'): nn.init.normal_(m.u, 0., 0.02)\n",
    "        if hasattr(m, 'v'): nn.init.normal_(m.v, 0., 0.02)\n",
    "\n",
    "tfmer_lm_config = dict(ctx_len=512, n_layers=12, n_heads=12, d_model=768, d_head=64, d_inner=3072, resid_p=0.1, attn_p=0.1,\n",
    "                         ff_p=0.1, embed_p=0.1, output_p=0., bias=True, scale=True, act=Activation.GeLU, double_drop=False,\n",
    "                         tie_weights=True, out_bias=False, init=init_transformer, mask=True)\n",
    "\n",
    "tfmer_clas_config = dict(ctx_len=512, n_layers=12, n_heads=12, d_model=768, d_head=64, d_inner=3072, resid_p=0.1, attn_p=0.1,\n",
    "                         ff_p=0.1, embed_p=0.1, output_p=0., bias=True, scale=True, act=Activation.GeLU, double_drop=False,\n",
    "                         init=init_transformer, mask=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from fastai.basics import *\n",
    "# from fastai.text.models.transformer import Activation, PositionalEncoding, feed_forward, init_transformer, _line_shift\n",
    "# from fastai.text.models.awd_lstm import RNNDropout\n",
    "from musicautobot.utils.attention_mask import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_multitask_model(vocab_size:int, config:dict=None, drop_mult:float=1., pad_idx=None):\n",
    "    \"Create a language model from `arch` and its `config`, maybe `pretrained`.\"\n",
    "    for k in config.keys(): \n",
    "        if k.endswith('_p'): config[k] *= drop_mult\n",
    "    n_hid = config['d_model']\n",
    "    mem_len = config.pop('mem_len')\n",
    "    embed = TransformerEmbedding(vocab_size, n_hid, embed_p=config['embed_p'], mem_len=mem_len, pad_idx=pad_idx)\n",
    "    encoder = MTEncoder(embed, n_hid, n_layers=config['enc_layers'], mem_len=0, **config) # encoder doesn't need memory\n",
    "    decoder = MTEncoder(embed, n_hid, is_decoder=True, n_layers=config['dec_layers'], mem_len=mem_len, **config)\n",
    "    head = MTLinearDecoder(n_hid, vocab_size, tie_encoder=embed.embed, **config)\n",
    "    model = MultiTransformer(encoder, decoder, head, mem_len=mem_len)\n",
    "    return model.apply(init_transformer)\n",
    "\n",
    "class MultiTransformer(nn.Module):\n",
    "    \"Multitask Transformer for training mask, next word, and sequence 2 sequence\"\n",
    "    def __init__(self, encoder, decoder, head, mem_len):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.head = head\n",
    "        self.default_mem_len = mem_len\n",
    "        self.current_mem_len = None\n",
    "    \n",
    "    def forward(self, inp):\n",
    "        # data order: mask, next word, melody, chord\n",
    "        outputs = {}\n",
    "        msk, lm, c2m, m2c = [inp.get(key) for key in ['msk', 'lm', 'c2m', 'm2c']]\n",
    "        \n",
    "        if msk is not None:\n",
    "            outputs['msk'] = self.head(self.encoder(msk['x'], msk['pos']))\n",
    "        if lm is not None:\n",
    "            outputs['lm'] = self.head(self.decoder(lm['x'], lm['pos']))\n",
    "        \n",
    "        if c2m is not None:\n",
    "            self.reset()\n",
    "            c2m_enc = self.encoder(c2m['enc'], c2m['enc_pos'])\n",
    "            c2m_dec = self.decoder(c2m['dec'], c2m['dec_pos'], c2m_enc)\n",
    "            outputs['c2m'] = self.head(c2m_dec)\n",
    "            \n",
    "        if m2c is not None:\n",
    "            self.reset()\n",
    "            m2c_enc = self.encoder(m2c['enc'], m2c['enc_pos'])\n",
    "            m2c_dec = self.decoder(m2c['dec'], m2c['dec_pos'], m2c_enc)\n",
    "            outputs['m2c'] = self.head(m2c_dec)\n",
    "            \n",
    "        return outputs\n",
    "    \n",
    "    \"A sequential module that passes the reset call to its children.\"\n",
    "    def reset(self):\n",
    "        for module in self.children(): \n",
    "            reset_children(module)\n",
    "        \n",
    "def reset_children(mod):\n",
    "    if hasattr(mod, 'reset'): mod.reset()\n",
    "    for module in mod.children(): \n",
    "        reset_children(module)\n",
    "\n",
    " # COMPONENTS\n",
    "class TransformerEmbedding(nn.Module):\n",
    "    \"Embedding + positional encoding + dropout\"\n",
    "    def __init__(self, vocab_size:int, emb_sz:int, embed_p:float=0., mem_len=512, beat_len=32, max_bar_len=1024, pad_idx=None):\n",
    "        super().__init__()\n",
    "        self.emb_sz = emb_sz\n",
    "        self.pad_idx = pad_idx\n",
    "        \n",
    "        self.embed = nn.Embedding(vocab_size, emb_sz, padding_idx=pad_idx)\n",
    "        self.pos_enc = PositionalEncoding(emb_sz)\n",
    "        self.beat_len, self.max_bar_len = beat_len, max_bar_len\n",
    "        self.beat_enc = nn.Embedding(beat_len, emb_sz, padding_idx=0)\n",
    "        self.bar_enc = nn.Embedding(max_bar_len, emb_sz, padding_idx=0)\n",
    "        \n",
    "        self.drop = nn.Dropout(embed_p)\n",
    "        self.mem_len = mem_len\n",
    "    \n",
    "    def forward(self, inp, pos):\n",
    "        beat_enc = self.beat_enc(pos % self.beat_len)\n",
    "        bar_pos = pos // self.beat_len % self.max_bar_len\n",
    "        bar_pos[bar_pos >= self.max_bar_len] = self.max_bar_len - 1\n",
    "        bar_enc = self.bar_enc((bar_pos))\n",
    "        emb = self.drop(self.embed(inp) + beat_enc + bar_enc)\n",
    "        return emb\n",
    "    \n",
    "    def relative_pos_enc(self, emb):\n",
    "#         return torch.arange(640-1, -1, -1).float().cuda()\n",
    "        seq_len = emb.shape[1] + self.mem_len\n",
    "        pos = torch.arange(seq_len-1, -1, -1, device=emb.device, dtype=emb.dtype) # backwards (txl pos encoding)\n",
    "        return self.pos_enc(pos)\n",
    "\n",
    "class MTLinearDecoder(nn.Module):\n",
    "    \"To go on top of a RNNCore module and create a Language Model.\"\n",
    "    initrange=0.1\n",
    "\n",
    "    def __init__(self, n_hid:int, n_out:int, output_p:float, tie_encoder:nn.Module=None, out_bias:bool=True, **kwargs):\n",
    "        super().__init__()\n",
    "        self.decoder = nn.Linear(n_hid, n_out, bias=out_bias)\n",
    "        self.decoder.weight.data.uniform_(-self.initrange, self.initrange)\n",
    "        self.output_dp = RNNDropout(output_p)\n",
    "        if out_bias: self.decoder.bias.data.zero_()\n",
    "        if tie_encoder: self.decoder.weight = tie_encoder.weight\n",
    "\n",
    "    def forward(self, input:Tuple[Tensor,Tensor])->Tuple[Tensor,Tensor,Tensor]:\n",
    "        output = self.output_dp(input)\n",
    "        decoded = self.decoder(output)\n",
    "        return decoded\n",
    "\n",
    "    \n",
    "# DECODER TRANSLATE BLOCK\n",
    "class MTEncoder(nn.Module):\n",
    "    def __init__(self, embed:nn.Module, n_hid:int, n_layers:int, n_heads:int, d_model:int, d_head:int, d_inner:int, \n",
    "                 resid_p:float=0., attn_p:float=0., ff_p:float=0., bias:bool=True, scale:bool=True,\n",
    "                 act:Activation=Activation.ReLU, double_drop:bool=True, mem_len:int=512, is_decoder=False,\n",
    "                 mask_steps=1, mask_p=0.3, **kwargs):\n",
    "        super().__init__()\n",
    "        self.embed = embed\n",
    "        self.u = nn.Parameter(torch.Tensor(n_heads, 1, d_head)) #Remove 1 for einsum implementation of attention\n",
    "        self.v = nn.Parameter(torch.Tensor(n_heads, 1, d_head)) #Remove 1 for einsum implementation of attention\n",
    "        self.n_layers,self.d_model = n_layers,d_model\n",
    "        self.layers = nn.ModuleList([MTEncoderBlock(n_heads, d_model, d_head, d_inner, resid_p=resid_p, attn_p=attn_p,\n",
    "                      ff_p=ff_p, bias=bias, scale=scale, act=act, double_drop=double_drop, mem_len=mem_len,\n",
    "                      ) for k in range(n_layers)])\n",
    "\n",
    "        self.mask_steps, self.mask_p = mask_steps, mask_p\n",
    "        self.is_decoder = is_decoder\n",
    "    \n",
    "        nn.init.normal_(self.u, 0., 0.02)\n",
    "        nn.init.normal_(self.v, 0., 0.02)\n",
    "        \n",
    "    def forward(self, x_lm, lm_pos, msk_emb=None):\n",
    "        bs,lm_len = x_lm.size()\n",
    "        \n",
    "        lm_emb = self.embed(x_lm, lm_pos)\n",
    "        if msk_emb is not None and msk_emb.shape[1] > lm_emb.shape[1]:\n",
    "            pos_enc = self.embed.relative_pos_enc(msk_emb)\n",
    "        else:\n",
    "            pos_enc = self.embed.relative_pos_enc(lm_emb)\n",
    "    \n",
    "        # Masks\n",
    "        if self.is_decoder:\n",
    "            lm_mask = rand_window_mask(lm_len, self.embed.mem_len, x_lm.device,\n",
    "                                       max_size=self.mask_steps, p=self.mask_p, is_eval=not self.training)\n",
    "        else:\n",
    "            lm_mask = None\n",
    "        \n",
    "        for i, layer in enumerate(self.layers):\n",
    "            lm_emb = layer(lm_emb, msk_emb, lm_mask=lm_mask,\n",
    "                        r=pos_enc, g_u=self.u, g_v=self.v)\n",
    "        return lm_emb\n",
    "\n",
    "class MTEncoderBlock(nn.Module):\n",
    "    \"Decoder block of a Transformer model.\"\n",
    "    #Can't use Sequential directly cause more than one input...\n",
    "    def __init__(self, n_heads:int, d_model:int, d_head:int, d_inner:int, resid_p:float=0., attn_p:float=0., ff_p:float=0.,\n",
    "                 bias:bool=True, scale:bool=True, double_drop:bool=True, mem_len:int=512, mha2_mem_len=0, **kwargs):\n",
    "        super().__init__()\n",
    "        attn_cls = MemMultiHeadRelativeAttentionKV\n",
    "        self.mha1 = attn_cls(n_heads, d_model, d_head, resid_p=resid_p, attn_p=attn_p, bias=bias, scale=scale, mem_len=mem_len, r_mask=False)\n",
    "        self.mha2 = attn_cls(n_heads, d_model, d_head, resid_p=resid_p, attn_p=attn_p, bias=bias, scale=scale, mem_len=mha2_mem_len, r_mask=True)\n",
    "        self.ff   = feed_forward(d_model, d_inner, ff_p=ff_p, double_drop=double_drop)\n",
    "    \n",
    "    def forward(self, enc_lm:Tensor, enc_msk:Tensor,\n",
    "                r=None, g_u=None, g_v=None,\n",
    "                msk_mask:Tensor=None, lm_mask:Tensor=None): \n",
    "        \n",
    "        y_lm = self.mha1(enc_lm, enc_lm, enc_lm, r, g_u, g_v, mask=lm_mask)\n",
    "        if enc_msk is None: return y_lm\n",
    "        return self.ff(self.mha2(y_lm, enc_msk, enc_msk, r, g_u, g_v, mask=msk_mask))\n",
    "\n",
    "# Attn\n",
    "\n",
    "class MemMultiHeadRelativeAttentionKV(nn.Module):\n",
    "    \"Attention Layer monster - relative positioning, keeps track of own memory, separate kv weights to support sequence2sequence decoding.\"\n",
    "    def __init__(self, n_heads:int, d_model:int, d_head:int=None, resid_p:float=0., attn_p:float=0., bias:bool=True,\n",
    "                 scale:bool=True, mem_len:int=512, r_mask=True):\n",
    "        super().__init__()\n",
    "        d_head = ifnone(d_head, d_model//n_heads)\n",
    "        self.n_heads,self.d_head,self.scale = n_heads,d_head,scale\n",
    "        \n",
    "        assert(d_model == d_head * n_heads)\n",
    "        self.q_wgt = nn.Linear(d_model, n_heads * d_head, bias=bias)\n",
    "        self.k_wgt = nn.Linear(d_model, n_heads * d_head, bias=bias)\n",
    "        self.v_wgt = nn.Linear(d_model, n_heads * d_head, bias=bias)\n",
    "        \n",
    "        self.drop_att,self.drop_res = nn.Dropout(attn_p),nn.Dropout(resid_p)\n",
    "        self.ln = nn.LayerNorm(d_model)\n",
    "        self.r_attn = nn.Linear(d_model, n_heads * d_head, bias=bias)\n",
    "        self.r_mask = r_mask\n",
    "\n",
    "        self.mem_len = mem_len\n",
    "        self.prev_k = None\n",
    "        self.prev_v = None\n",
    "        \n",
    "    def forward(self, q:Tensor, k:Tensor=None, v:Tensor=None, \n",
    "                r:Tensor=None, g_u:Tensor=None, g_v:Tensor=None, \n",
    "                mask:Tensor=None, **kwargs):\n",
    "        if k is None: k = q\n",
    "        if v is None: v = q\n",
    "        return self.ln(q + self.drop_res(self._apply_attention(q, k, v, r, g_u, g_v, mask=mask, **kwargs)))\n",
    "\n",
    "    def mem_k(self, k):\n",
    "        if self.mem_len == 0: return k\n",
    "        if self.prev_k is None or (self.prev_k.shape[0] != k.shape[0]): # reset if wrong batch size\n",
    "            self.prev_k = k[:, -self.mem_len:]\n",
    "            return k\n",
    "        with torch.no_grad():\n",
    "            k_ext = torch.cat([self.prev_k, k], dim=1)\n",
    "            self.prev_k = k_ext[:, -self.mem_len:]\n",
    "        return k_ext.detach()\n",
    "    \n",
    "    def mem_v(self, v):\n",
    "        if self.mem_len == 0: return v\n",
    "        if self.prev_v is None or (self.prev_v.shape[0] != v.shape[0]): # reset if wrong batch size\n",
    "            self.prev_v = v[:, -self.mem_len:]\n",
    "            return v\n",
    "        with torch.no_grad():\n",
    "            v_ext = torch.cat([self.prev_v, v], dim=1)\n",
    "            self.prev_v = v_ext[:, -self.mem_len:]\n",
    "        return v_ext.detach()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.prev_v = None\n",
    "        self.prev_k = None\n",
    "        \n",
    "    def _apply_attention(self, q:Tensor, k:Tensor, v:Tensor, \n",
    "                         r:Tensor=None, g_u:Tensor=None, g_v:Tensor=None, \n",
    "                         mask:Tensor=None, **kwargs):\n",
    "        #Notations from the paper: x input, r vector of relative distance between two elements, u et v learnable\n",
    "        #parameters of the model common between all layers, mask to avoid cheating and mem the previous hidden states.\n",
    "#         bs,x_len,seq_len = q.size(0),q.size(1),r.size(0)\n",
    "        k = self.mem_k(k)\n",
    "        v = self.mem_v(v)\n",
    "        bs,x_len,seq_len = q.size(0),q.size(1),k.size(1)\n",
    "        wq,wk,wv = self.q_wgt(q),self.k_wgt(k),self.v_wgt(v)\n",
    "        wq = wq[:,-x_len:]\n",
    "        wq,wk,wv = map(lambda x:x.view(bs, x.size(1), self.n_heads, self.d_head), (wq,wk,wv))\n",
    "        wq,wk,wv = wq.permute(0, 2, 1, 3),wk.permute(0, 2, 3, 1),wv.permute(0, 2, 1, 3)\n",
    "        wkr = self.r_attn(r[-seq_len:])\n",
    "        wkr = wkr.view(seq_len, self.n_heads, self.d_head)\n",
    "        wkr = wkr.permute(1,2,0)\n",
    "        #### compute attention score (AC is (a) + (c) and BS is (b) + (d) in the paper)\n",
    "        AC = torch.matmul(wq+g_u,wk)\n",
    "        BD = _line_shift(torch.matmul(wq+g_v, wkr), mask=self.r_mask)\n",
    "        if self.scale: attn_score = (AC + BD).mul_(1/(self.d_head ** 0.5))\n",
    "        if mask is not None: \n",
    "            mask = mask[...,-seq_len:]\n",
    "            if hasattr(mask, 'bool'): mask = mask.bool()\n",
    "            attn_score = attn_score.float().masked_fill(mask, -float('inf')).type_as(attn_score)\n",
    "        attn_prob = self.drop_att(F.softmax(attn_score, dim=-1))\n",
    "        attn_vec = torch.matmul(attn_prob, wv)\n",
    "        return attn_vec.permute(0, 2, 1, 3).contiguous().view(bs, x_len, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted config.ipynb.\n",
      "Converted Train-before_cleanup.ipynb.\n",
      "Converted Train.ipynb.\n",
      "Converted dataloader.ipynb.\n",
      "Converted learner.ipynb.\n",
      "Converted model.ipynb.\n",
      "Converted transform.ipynb.\n",
      "Converted Train-Scratch.ipynb.\n",
      "Converted Train.ipynb.\n",
      "Converted dataloader-reference.ipynb.\n",
      "Converted dataloader-v1.ipynb.\n",
      "Converted dataloader.ipynb.\n",
      "Converted learner.ipynb.\n",
      "Converted model.ipynb.\n",
      "Converted transform.ipynb.\n",
      "Converted numpy_encode.ipynb.\n",
      "Converted attention_mask.ipynb.\n",
      "Converted env_setup.ipynb.\n",
      "Converted file_processing.ipynb.\n",
      "Converted lamb.ipynb.\n",
      "Converted midifile.ipynb.\n",
      "Converted stacked_dataloader.ipynb.\n",
      "Converted top_k_top_p.ipynb.\n",
      "Converted vocab.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script(recursive=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
